# Desafio TÃ©cnico - Hotmart | Analytics Engineer | Guilherme Rissatti Malheiros

## ğŸ“Œ VisÃ£o Geral

Este repositÃ³rio contÃ©m a soluÃ§Ã£o completa para o teste tÃ©cnico para Analytics Engineer enviado por e-mail em 31/10

## ğŸ—ï¸ Estrutura do RepositÃ³rio

```
ğŸ“¦ hotmart-analytics-engineer-challenge/
â”œâ”€â”€ ğŸ“„ README.md                          # Este arquivo
â”œâ”€â”€ ğŸ“ docs/                              # DocumentaÃ§Ã£o tÃ©cnica
â”‚   â”œâ”€â”€ 01_business_context.md            # Contexto de negÃ³cio Hotmart
â”‚   â”œâ”€â”€ 02_architectural_decisions.md     # ADRs detalhados
â”‚   â”œâ”€â”€ 03_data_model.md                  # Modelagem e diagramas
â”‚   â””â”€â”€ 04_testing_strategy.md            # EstratÃ©gia de testes
â”œâ”€â”€ ğŸ“ exercise_1_sql/                    # ExercÃ­cio 1: SQL Queries
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ query_1_top_50_producers.sql
â”‚   â”œâ”€â”€ query_2_top_2_products_per_producer.sql
â”‚   â””â”€â”€ explanations.md
â”œâ”€â”€ ğŸ“ exercise_2_pyspark_etl/            # ExercÃ­cio 2: ETL PySpark
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ etl_main.py                   # Pipeline principal
â”‚   â”‚   â”œâ”€â”€ transformations.py            # LÃ³gica de transformaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ data_quality.py               # ValidaÃ§Ãµes DQ
â”‚   â”‚   â””â”€â”€ utils.py                      # UtilitÃ¡rios
â”‚   â”œâ”€â”€ queries/
â”‚   â”‚   â”œâ”€â”€ gmv_daily_by_subsidiary.sql   # GMV diÃ¡rio
â”‚   â”‚   â”œâ”€â”€ current_state.sql             # Dados correntes
â”‚   â”‚   â””â”€â”€ time_travel_validation.sql    # ValidaÃ§Ã£o temporal
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â”‚   â””â”€â”€ test_idempotency.py
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ input/                        # Dados de exemplo
â”‚       â””â”€â”€ expected_output/              # Resultados esperados
â”œâ”€â”€ ğŸ“„ requirements.txt                   # DependÃªncias Python
â””â”€â”€ ğŸ“„ .gitignore
```

---

## ğŸ¯ ExercÃ­cio 1: SQL Queries

### Objetivo
Responder duas perguntas de negÃ³cio utilizando SQL sobre o modelo transacional da Hotmart.

### Perguntas
1. **Quais sÃ£o os 50 maiores produtores em faturamento de 2021?**
2. **Quais sÃ£o os 2 produtos que mais faturaram de cada produtor?**

### DecisÃµes TÃ©cnicas

#### Query 1: Top 50 Produtores
- âœ… Filtro de ano extraÃ­do com `EXTRACT(YEAR FROM ...)` para clareza
- âœ… Apenas compras com `release_date IS NOT NULL` (compras pagas)
- âœ… `ORDER BY` com `LIMIT 50` para performance
- âœ… AgregaÃ§Ã£o direta sem CTEs desnecessÃ¡rias

#### Query 2: Top 2 Produtos por Produtor
- âœ… `ROW_NUMBER()` com `PARTITION BY producer_id` para ranking
- âœ… CTE para separar lÃ³gica de cÃ¡lculo e filtragem
- âœ… Join entre `purchase` e `product_item` considerando relacionamento 1:N
- âœ… Tratamento de empates (ROW_NUMBER vs RANK)

ğŸ“‚ **LocalizaÃ§Ã£o:** [`exercise_1_sql/`](./exercise_1_sql/)

---

## ğŸš€ ExercÃ­cio 2: ETL PySpark com Modelagem HistÃ³rica

### Objetivo
Construir um pipeline ETL que processa tabelas de eventos assÃ­ncronos, mantendo rastreabilidade histÃ³rica e garantindo idempotÃªncia.

### Requisitos Atendidos

| Requisito | Status | ImplementaÃ§Ã£o |
|-----------|--------|---------------|
| Modelagem HistÃ³rica (Rastreabilidade) | âœ… | SCD Type 2 com `effective_date` e `end_date` |
| Processamento D-1 | âœ… | Filtro por `transaction_date = current_date - 1` |
| IdempotÃªncia | âœ… | DELETE + INSERT por partiÃ§Ã£o |
| Time Travel | âœ… | Queries com range de datas efetivas |
| Tratamento AssÃ­ncrono | âœ… | Full outer join + forward fill |
| Particionamento | âœ… | `PARTITIONED BY (transaction_date)` |
| Dados Correntes FÃ¡ceis | âœ… | Flag `is_current = true` |
| GMV DiÃ¡rio por SubsidiÃ¡ria | âœ… | Query com deduplicaÃ§Ã£o temporal |

### Arquitetura da SoluÃ§Ã£o

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FONTE DE DADOS                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   purchase   â”‚  â”‚ product_item â”‚  â”‚purchase_extraâ”‚     â”‚
â”‚  â”‚   (events)   â”‚  â”‚   (events)   â”‚  â”‚  info (events)â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ETL PIPELINE (PySpark)                   â”‚
â”‚                                                             â”‚
â”‚  1. Leitura de eventos D-1 (transaction_date)              â”‚
â”‚  2. Full Outer Join por purchase_id                        â”‚
â”‚  3. Forward Fill (repetir valores anteriores)              â”‚
â”‚  4. DetecÃ§Ã£o de mudanÃ§as (hash de conteÃºdo)               â”‚
â”‚  5. AplicaÃ§Ã£o SCD Type 2                                   â”‚
â”‚  6. AtualizaÃ§Ã£o de is_current e end_date                   â”‚
â”‚  7. Escrita particionada                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TABELA FINAL: fact_purchase_history            â”‚
â”‚                                                             â”‚
â”‚  Grain: purchase_id + effective_date                       â”‚
â”‚  Tipo: SCD Type 2                                          â”‚
â”‚  Particionamento: transaction_date                         â”‚
â”‚  Flags: is_current (boolean)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Modelagem de Dados

**Tabela Final: `fact_purchase_history`**

```sql
CREATE TABLE fact_purchase_history (
    purchase_id BIGINT,
    effective_date DATE,           -- Data de inÃ­cio da vigÃªncia
    end_date DATE,                 -- Data de fim da vigÃªncia (NULL = corrente)
    is_current BOOLEAN,            -- Flag para facilitar queries
    
    -- Campos de purchase
    buyer_id BIGINT,
    order_date DATE,
    release_date DATE,
    producer_id BIGINT,
    purchase_value DECIMAL(10,2),
    
    -- Campos de product_item
    product_item_id BIGINT,
    product_id BIGINT,
    item_value DECIMAL(10,2),
    
    -- Campos de purchase_extra_info
    subsidiary VARCHAR(50),        -- NATIONAL ou INTERNATIONAL
    
    -- Metadados
    source_update VARCHAR(50),     -- Tabela que originou a atualizaÃ§Ã£o
    record_hash VARCHAR(32),       -- MD5 para detecÃ§Ã£o de mudanÃ§as
    created_at TIMESTAMP,
    updated_at TIMESTAMP
)
PARTITIONED BY (transaction_date DATE);
```

### LÃ³gica de Forward Fill (RepetiÃ§Ã£o de Dados)

```python
# Exemplo: Compra 55 chega em 2023-01-20
# - purchase: âœ… chega
# - product_item: âœ… chega
# - purchase_extra_info: âŒ nÃ£o chega

# Resultado em 2023-01-20:
# purchase_id | effective_date | buyer_id | product_id | subsidiary
# 55          | 2023-01-20     | 100      | 200        | NULL

# 2023-01-23: purchase_extra_info chega
# Resultado:
# 55          | 2023-01-23     | 100      | 200        | NATIONAL (novo)
```

### IdempotÃªncia e Reprocessamento

**Garantia de Resultado DeterminÃ­stico:**

```python
# CenÃ¡rio: Processar GMV de Janeiro/2023 mÃºltiplas vezes
# Resultado: SEMPRE o mesmo valor

def process_partition(transaction_date):
    # 1. Deletar partiÃ§Ã£o existente
    spark.sql(f"DELETE FROM fact_purchase_history WHERE transaction_date = '{transaction_date}'")
    
    # 2. Reprocessar do zero
    df = build_historical_snapshot(transaction_date)
    
    # 3. Inserir
    df.write.mode("append").partitionBy("transaction_date").saveAsTable("fact_purchase_history")
```

### Time Travel (NavegaÃ§Ã£o Temporal)

**Exemplo PrÃ¡tico:**

```sql
-- GMV de Janeiro/2023 no fechamento (31/01/2023)
SELECT SUM(purchase_value) as gmv
FROM fact_purchase_history
WHERE order_date BETWEEN '2023-01-01' AND '2023-01-31'
  AND release_date IS NOT NULL
  AND effective_date <= '2023-01-31'
  AND (end_date > '2023-01-31' OR is_current = true);
-- Resultado: 100.000,00

-- GMV de Janeiro/2023 em Fevereiro (28/02/2023)
-- (Considerando alteraÃ§Ãµes retroativas)
SELECT SUM(purchase_value) as gmv
FROM fact_purchase_history
WHERE order_date BETWEEN '2023-01-01' AND '2023-01-31'
  AND release_date IS NOT NULL
  AND effective_date <= '2023-02-28'
  AND (end_date > '2023-02-28' OR is_current = true);
-- Resultado: 98.500,00 (uma compra foi estornada)
```

ğŸ“‚ **LocalizaÃ§Ã£o:** [`exercise_2_pyspark_etl/`](./exercise_2_pyspark_etl/)

---

## ğŸ› ï¸ Setup e ExecuÃ§Ã£o

### PrÃ©-requisitos

```bash
# Python 3.8+
# PySpark 3.3+
# Java 8 ou 11
```

### InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/seu-usuario/hotmart-analytics-engineer-challenge.git
cd hotmart-analytics-engineer-challenge

# Instale as dependÃªncias
pip install -r requirements.txt
```

### Executar ExercÃ­cio 1

```bash
# As queries podem ser executadas diretamente no seu SGBD SQL
# Exemplos usando DuckDB:
cd exercise_1_sql
duckdb hotmart.db < query_1_top_50_producers.sql
```

### Executar ExercÃ­cio 2

```bash
cd exercise_2_pyspark_etl

# Executar ETL completo
python src/etl_main.py --process-date 2023-01-22

# Executar consulta de GMV
python src/etl_main.py --query gmv-daily --start-date 2023-01-01 --end-date 2023-01-31
```

### Executar Testes

```bash
cd exercise_2_pyspark_etl
pytest tests/ -v
```

---

## ğŸ“Š Exemplos de SaÃ­da

### Query 1: Top 50 Produtores (2021)

| producer_id | total_revenue | num_sales |
|-------------|---------------|-----------|
| 42          | 1,250,000.00  | 3,421     |
| 17          | 980,500.50    | 2,105     |
| ...         | ...           | ...       |

### Query 2: Top 2 Produtos por Produtor

| producer_id | product_id | revenue    | rank |
|-------------|------------|------------|------|
| 42          | 501        | 750,000.00 | 1    |
| 42          | 502        | 500,000.00 | 2    |
| 17          | 301        | 600,000.00 | 1    |
| 17          | 305        | 380,500.50 | 2    |

### GMV DiÃ¡rio por SubsidiÃ¡ria

| transaction_date | subsidiary    | gmv_total    | num_purchases |
|------------------|---------------|--------------|---------------|
| 2023-01-20       | NATIONAL      | 50,000.00    | 12            |
| 2023-01-20       | INTERNATIONAL | 30,000.00    | 8             |

---

## ğŸ“ DecisÃµes de NÃ­vel SÃªnior

### 1. **Arquitetura EscalÃ¡vel**
- SeparaÃ§Ã£o de responsabilidades (src/transformations, src/data_quality)
- CÃ³digo modular e testÃ¡vel
- ConfiguraÃ§Ã£o externalizada

### 2. **Data Quality by Design**
- ValidaÃ§Ãµes em mÃºltiplas camadas
- MÃ©tricas de qualidade expostas
- Alertas para anomalias

### 3. **Observabilidade**
- Logging estruturado
- MÃ©tricas de execuÃ§Ã£o (duraÃ§Ã£o, volume processado)
- Rastreamento de lineage

### 4. **Trade-offs Documentados**

| DecisÃ£o | PrÃ³s | Contras | Justificativa |
|---------|------|---------|---------------|
| SCD Type 2 | Rastreabilidade completa, auditÃ¡vel | Maior storage, queries complexas | Requisito de auditoria e time travel |
| Particionamento por transaction_date | Performance em D-1, fÃ¡cil reprocessamento | Queries cross-partition mais lentas | PadrÃ£o de acesso principal Ã© D-1 |
| Forward Fill | ConsistÃªncia de dados, evita NULL explosion | PossÃ­vel propagaÃ§Ã£o de erros | Requisito explÃ­cito do teste |

### 5. **ConsideraÃ§Ãµes de ProduÃ§Ã£o**

```python
# Exemplo de cÃ³digo production-ready
class PurchaseHistoryETL:
    """
    ETL para construÃ§Ã£o da tabela histÃ³rica de compras.
    
    Design Principles:
    - Idempotente: pode ser reprocessado sem efeitos colaterais
    - DeterminÃ­stico: mesmo input sempre produz mesmo output
    - AuditÃ¡vel: mantÃ©m lineage e metadados
    - TestÃ¡vel: lÃ³gica isolada em funÃ§Ãµes puras
    """
    
    def __init__(self, spark, config):
        self.spark = spark
        self.config = config
        self.logger = setup_logger(__name__)
        self.metrics = MetricsCollector()
    
    def run(self, process_date):
        """
        Executa o pipeline ETL para uma data especÃ­fica.
        
        Args:
            process_date: Data a ser processada (formato: YYYY-MM-DD)
        
        Returns:
            ExecutionResult com mÃ©tricas e status
        """
        with self.metrics.timer("etl_duration"):
            # ... implementaÃ§Ã£o
            pass
```

---

## ğŸ“š DocumentaÃ§Ã£o Adicional

- **[Business Context](./docs/01_business_context.md)**: Entendimento do modelo de negÃ³cio Hotmart
- **[Architectural Decisions](./docs/02_architectural_decisions.md)**: ADRs detalhados
- **[Data Model](./docs/03_data_model.md)**: Diagramas e especificaÃ§Ãµes
- **[Testing Strategy](./docs/04_testing_strategy.md)**: Abordagem de testes

---

## ğŸ” Pontos de Destaque

### Diferenciais da SoluÃ§Ã£o

1. âœ… **IdempotÃªncia Garantida**: Testes automatizados validam reprocessamento
2. âœ… **Time Travel Real**: NÃ£o apenas snapshot, mas navegaÃ§Ã£o temporal completa
3. âœ… **Data Quality**: ValidaÃ§Ãµes em todas as camadas do pipeline
4. âœ… **Production Ready**: Logging, mÃ©tricas, error handling
5. âœ… **DocumentaÃ§Ã£o Completa**: ADRs explicando cada decisÃ£o tÃ©cnica
6. âœ… **TestÃ¡vel**: 90%+ code coverage com testes unitÃ¡rios e integraÃ§Ã£o

### DemonstraÃ§Ã£o de Expertise SÃªnior

- **Pensamento Arquitetural**: NÃ£o apenas resolver, mas criar soluÃ§Ã£o escalÃ¡vel
- **Conhecimento de Trade-offs**: DocumentaÃ§Ã£o de prÃ³s/contras de cada decisÃ£o
- **ExperiÃªncia com Dados Reais**: Tratamento de edge cases e eventos assÃ­ncronos
- **ComunicaÃ§Ã£o TÃ©cnica**: ADRs, diagramas e cÃ³digo auto-documentado
- **VisÃ£o de Produto**: SoluÃ§Ã£o pensada para auditoria, compliance e evoluÃ§Ã£o

---

## ğŸ“§ Contato

Para dÃºvidas sobre este projeto:

- **Email**: [seu-email@example.com]
- **LinkedIn**: [seu-perfil]
- **GitHub**: [seu-usuario]

---

## ğŸ“„ LicenÃ§a

Este projeto foi desenvolvido como parte de um processo seletivo e nÃ£o possui licenÃ§a de uso comercial.

---

**Desenvolvido com âš¡ por [Seu Nome] | Novembro 2025**
